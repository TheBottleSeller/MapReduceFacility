\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{enumerate}

\oddsidemargin0cm
\topmargin-2cm 
\textwidth16.5cm
\textheight23.5cm  

\newcommand{\question}[2] {\vspace{.25in} \hrule\vspace{0.5em}
\noindent{\bf #1: #2} \vspace{0.5em}
\hrule \vspace{.10in}}
\renewcommand{\part}[1] {\vspace{.10in} {\bf (#1)}}

\newcommand{\qed}{\hfill \ensuremath{\Box}}

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}

\begin{document}

\medskip

\thispagestyle{plain}
\begin{center}
{\Large 15-440 Lab 3: MapReduce} \\
Neil Batlivala and Neha Rathi \\
November 16, 2013 \\
\end{center}

\question{I} {Design}
A map reduce cluster is initiated at the master machine and is configured using a config file. Once run, the master ssh's into each of the participant machines, runs a FacilityMananger on each of the machines and waits until the machines have all connected, and once they have, the master allows for commands to be typed into a supplied command prompt.

The distributed file system is comprised of two socket servers, one Read server and one Writer server. The Read server allows for other remote machines to read files from the local machine whereas the Write server allows for other machines to write files to the local machine. We made this decision because portions of the map reduce flow are made easier using push and pull models, for example when uploading a file, the local machine containing the file can push their data onto other machines via the Write server on other machines, whereas when a Reducer needs a partition file, it can pull the data from another machine via the Read server on the other machines. This was our attempt at creating a distributed IO facility. The distributed file system also creates local directories in the /tmp/data440 file on each machine (to ensure that the data is not shared over AFS). Also, care was taken to ensure that when uploading a file onto the distributed file system, if a node is not reached, then the block that was supposed to be distributed to that node will be redistributed to another node. Apart from this, there is limited functionality to the ditsributed file system with respect to the programmer. He/she can simply upload files, which are replicated and available to use by a map reduce program. 

The MapReduce facility is run by a FacilityManger which can be either a regular FacilityManager or a FacilityManagerMaster. We used RMI to provide remote access to each of the facility managers across the network. In this way, we leveraged RMI to send simple messages back and forth between managers. Although the overhead for this may be slightly more than having direct socket connections, we believe that the ease of code comprehension using RMI (i.e. FacilityManger's look and behave as if they were local) is greater than the having slightly more efficient code (with respect to the amount of data sent over the wire). From the application programmer's perspective, this allows for a master to be started up, and then can reach out to the registries of the other participants to lookup the remote facility managers. RMI is also a robust, enterprise tested tool. 

To use the map reduce facility, a programmer must write a java class that extends MapReduce440 which contains two classes, one that extends Mapper440 and another that extends Reducer440. Both have a map/reduce function that must be implemented. The map reduce facility only supports Strings as input types, where each record is viewed as a single line of the input file, and so the map function from Mapper440 takes a single String as a record and outputs a list of KeyValue pairs. All intermediatary results are also dealt with as Strings, and thus the programmer is restricted to this as well. However, the programmer can always parse to and from strings, so it should not be too inhibiting. Also, there are two functions that must be created in the MapReduce440 class that the programmer writes, and these functions must instantiate their mapper and a reducer respectively.

A single MapReduce program is carried out by 4 different types of jobs: a MapJob, MapCombineJob, ReduceJob, and ReduceCombineJob. The MapJob pertains to running a map across a single data block, a MapCombine job pertains to combining all the mapped blocks on a single node, a ReduceJob pertains to pulling all the partitions from the mappers onto a local machines, merging the partitions, and then running a reduce on the merged partition, and the ReduceCombineJob gathers all the reduces outputs onto a single machine and concatenates them together. 

Config:
\\ The config file allows for the file parameters of the MapReduce cluster to be customized:
\\ - cluster name (the name supplied to cluster so that multiple clusters can share machines)
\\ - master ip address
\\ - participants ip addresses (must contain master ip address)
\\ - file system read port (the port on which the read server will listen)
\\ - file system write port (the port on which the write server will lsiten)
\\ - rmi port (the port on which the RMI registry will be created)
\\ - maximum number of maps per host 
\\ - maximum number of reduces per host 
\\ - replication factor
\\ - block size (note that because we deal with records as a single line of text, the block size is the number of lines to be included in each block, which does not necessarily keep the size of each block in memory consistent, but our assumption of String line records required this simplification).

Health Checker
\\ To ensure that everything is running smoothly in the system, a health checker is created on the master that pings all the participants every 2 seconds to ensure that they are alive. If they do not respond, then the participant is considered to have died and the jobs running on that node are reasiigned. 

MapReduceProgram
\\ A map reduce program is an object that gets instantiated to handle the state of a single map reduce program run by a user. It is only this object that can isntantiate new Map/Reduce/MapCombine/ReduceCombine jobs and it is this object that keeps track of which nodes are running which jobs that are required for this program's completion. In this way, we use the MapReduceProgram as a source of truth on the master for a map reduce program. This was an important concept that we used to ensure that jobs were completed, or re-dispatched in case of failure.

Job Dispatcher
\\ To keep control over the jobs running in the system, all created jobs (MapJob, ReduceJob, etc.) are added to a blocking queue of the Job Dispatcher. When available, the dispatcher will attempt to find a worker to run that job (see Job Scheduler below), and will dispatch that job to the appropriate manager. A lot of care was taken between the interactions of the Job Dispatcher and the Job Scheduler to ensure that jobs are scheduled to nodes based on data locality first, and that the MAX_REDUCES and MAX_MAPS per host are not exceeded. Essentially when the Map Reduce facility is operating at full capacity, Jobs will migrate between the dispatcher and scheduler, while trying to be assigned a valid participant worker.

Job Scheduler
\\ The job scheduler handles all the scheduling of jobs. In the case of MapJobs it tries to find first a participant that has the required data block and ensures that the chosen worker is operating under the maximum map task capacity. For ReduceJobs, only the maximum reduce task capacity is ensured to be maintained when choosing a participant worker for some job. Note that if no participant can be found, the job is just re-enqued into the Job Dispatcher's blocking queue (thus at fully capacity, a job will cycle between the dispatcher and scheduler). One major simplifcation that is made with respect to the reduce phase is that the number of reducers for any particular user-program is equal to the total number of participants. This may not be the best, and in fact the number of reducers should be dependent on the utilization of the system, however we decided to forego this and just assume that every participant is going to eventually become a reducer for every user-program.

Record Reader and Record Writer
\\ These classes handle the reading and writing of the intermediary files. The format for writing a key value pair just uses a new line as a delimter. When reading and writing key's with multiple values, the key, number of values, and list of values are all written using a newline as a delimeter. This class is failry straightforward considering that only String types are handled by the facility.

\question{II} {Implementation}
% Describe the portions of the design that are correctly implemented, that have bugs, and that remain unimplemented.
The Map Reduce facility will properly boot up, and will reach out to all the 

\question{III} {Build, Deploy, and Run}
% Tell us how to cleanly build, deploy, and run your project.

We have provided a config file template.

First, run the master using the following command: 

\begin{center}\texttt{java -jar MapReduceFacility.jar -m <config-file>}\end{center}

Then, programs can be executed from a participant (using the following command:

\begin{center}\texttt{java -jar MapReduceFacility.jar -p <port> -c <cluster-name>}\end{center}

Where \texttt{port} is the MR port that was specified in the config file passed to the master and \texttt{cluster-name} is the cluster name that was specified in the config file passed to the master.

\question{IV} {Dependencies}
% Highlight any dependencies and software or system requirements. 

Our project depends on Java 1.6.0 or higher.

\question{V} {Running and Testing}
% Tell us how to run and test your framework.

Running \texttt{MainServer} will create a registry local to the server's machine and create/bind two \texttt{TestObject}s to this registry. Then, running \texttt{MainClient -c <hostname>} will create a registry local to the client's machine, get the registry local to the server's machine, create/bind two \texttt{TestObject}s to the former, and lookup the two \texttt{TestObjects} from the latter. After this, a series of 17 tests will be invoked on one stub using the two local objects and other stub. As each test passes, a corresponding message will be printed out. Here, we will describe these tests in detail: \\

\textbf{Test 1 (add)}:



\end{document}